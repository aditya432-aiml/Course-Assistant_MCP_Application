{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c703ff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\adity\\miniconda3\\envs\\MCP_Course_Assistant\\lib\\site-packages\\unstructured\\partition\\pdf.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTContainer, LTImage, LTItem, LTTextBox\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_filename\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpi_heif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_heif_opener\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "PDF_FOLDER = \"Data_Training/15_06_2025/\"\n",
    "VECTOR_DIR = \"vector_store\"\n",
    "FAISS_INDEX_FILE = os.path.join(VECTOR_DIR, \"index.faiss\")\n",
    "CHUNKS_FILE = os.path.join(VECTOR_DIR, \"chunks.pkl\")\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaff3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 1: PDF Text Extraction ==========\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    elements = partition_pdf(str(pdf_path))\n",
    "    return \"\\n\".join([el.text for el in elements if hasattr(el, \"text\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: Clean Text ==========\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 3: Chunking ==========\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 4: Embed Text ==========\n",
    "def embed_chunks(chunks, model):\n",
    "    return model.encode(chunks, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 5: Save FAISS Index and Chunks ==========\n",
    "def save_vector_store(index, chunks):\n",
    "    os.makedirs(VECTOR_DIR, exist_ok=True)\n",
    "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
    "    with open(CHUNKS_FILE, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 6: Load FAISS Index and Chunks ==========\n",
    "def load_vector_store():\n",
    "    index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "    with open(CHUNKS_FILE, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ac784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 7: Retrieve Top-K Chunks ==========\n",
    "def retrieve_chunks(question, model, index, chunks, top_k=5):\n",
    "    query_embedding = model.encode([question], convert_to_tensor=False).astype(\"float32\")\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    return [chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd68241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MAIN PROCESS ==========\n",
    "def process_pdfs_and_store():\n",
    "    print(\"‚è≥ Loading embedding model...\")\n",
    "    embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    all_chunks, all_embeddings = [], []\n",
    "    pdf_files = list(Path(PDF_FOLDER).glob(\"*.pdf\"))\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"üìÑ Processing: {pdf_file.name}\")\n",
    "        raw_text = extract_text_from_pdf(pdf_file)\n",
    "        cleaned = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned)\n",
    "        embeddings = embed_chunks(chunks, embed_model)\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    # Convert and store\n",
    "    embedding_matrix = np.vstack(all_embeddings).astype(\"float32\")\n",
    "    index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "    index.add(embedding_matrix)\n",
    "\n",
    "    save_vector_store(index, all_chunks)\n",
    "    print(\"‚úÖ All done! FAISS index and chunks saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== OPTIONAL: Query Interface ==========\n",
    "def ask_question():\n",
    "    print(\"‚è≥ Loading model and vector store...\")\n",
    "    embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    index, chunks = load_vector_store()\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nüí¨ Ask a question (or type 'exit'): \")\n",
    "        if question.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        top_chunks = retrieve_chunks(question, embed_model, index, chunks, top_k=5)\n",
    "        print(\"\\nüìå Top Relevant Chunks:\\n\")\n",
    "        for i, chunk in enumerate(top_chunks, 1):\n",
    "            print(f\"--- Chunk {i} ---\\n{chunk}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCP_Course_Assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
