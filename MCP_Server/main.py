from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from duckduckgo_search import DDGS
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from typing import List
import os
import pickle
import numpy as np
import faiss

load_dotenv()

# ========== MCP Init ==========
mcp = FastMCP("docs")

# ========== OpenAI Docker Client ==========
client = OpenAI(
    base_url="http://localhost:12434/engines/v1",
    api_key="docker"
)

# ========== Web Search ==========
def search_web_duckduckgo(query: str, max_results=5):
    search_results = ""
    with DDGS() as ddgs:
        results = ddgs.text(query, region='wt-wt', safesearch='Moderate', max_results=max_results)
        for r in results:
            search_results += f"{r.get('title')}\n{r.get('body')}\n{r.get('href')}\n\n"
    return search_results.strip()

# ========== LLM Query ==========
def query_llm_with_results(question: str, context: str) -> str:
    response = client.chat.completions.create(
        model="ai/llama3.2:1B-Q4_0",
        messages=[
            {
                "role": "system",
                "content": f"You are a helpful assistant. Use only the provided context:\n\n{context}"
            },
            {
                "role": "user",
                "content": question
            }
        ]
    )
    return response.choices[0].message.content

# ========== Tool 1: General Purpose Web-Based Docs Search ==========
@mcp.tool()
async def get_docs(query: str) -> str:
    """
    Tool 1: Perform general-purpose documentation/web search and return LLM-based answer.

    Args:
      query: Documentation-related question or concept.

    Returns:
      Response generated by LLM using web context.
    """
    search_results = search_web_duckduckgo(query, max_results=5)
    if not search_results:
        return "⚠️ No relevant results found on the web."
    return query_llm_with_results(query, search_results)


# ========== Tool 2: Retrieval from FAISS Vector Store ==========
VECTOR_DIR = "Data_Extraction_and_Retrival/vector_store"
FAISS_INDEX_FILE = os.path.join(VECTOR_DIR, "index.faiss")
CHUNKS_FILE = os.path.join(VECTOR_DIR, "chunks.pkl")
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

def load_vector_store():
    index = faiss.read_index(FAISS_INDEX_FILE)
    with open(CHUNKS_FILE, "rb") as f:
        chunks = pickle.load(f)
    return index, chunks

def retrieve_chunks(question: str, model, index, chunks, top_k=5) -> List[str]:
    query_embedding = model.encode([question], convert_to_tensor=False).astype("float32")
    D, I = index.search(np.array(query_embedding), top_k)
    return [chunks[i] for i in I[0]]

@mcp.tool()
async def retrieval_tool(question: str) -> str:
    """
    Tool 2: Retrieve top-k chunks from FAISS + Answer via LLM.
    """
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        index, chunks = load_vector_store()
        top_chunks = retrieve_chunks(question, model, index, chunks, top_k=5)

        if not top_chunks:
            return "⚠️ No relevant content found."

        context = "\n\n".join(top_chunks)
        return query_llm_with_results(question, context)
    except Exception as e:
        return f"❌ Retrieval error: {str(e)}"


# ========== Tool 3: General Web Search Answer ==========
@mcp.tool()
async def web_search_tool(question: str) -> str:
    """
    Tool 3: Use DuckDuckGo search + LLM for open-ended web queries.
    """
    try:
        search_results = search_web_duckduckgo(question, max_results=5)
        if not search_results:
            return "⚠️ No web search results found."
        return query_llm_with_results(question, search_results)
    except Exception as e:
        return f"❌ Web search error: {str(e)}"


# ========== Run MCP Server ==========
if __name__ == "__main__":
    mcp.run(transport="stdio")